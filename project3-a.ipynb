{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= 'red'>Multi Layer Perceptron</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.2810 - acc: 0.9207 - val_loss: 0.1413 - val_acc: 0.9576\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1115 - acc: 0.9677 - val_loss: 0.0911 - val_acc: 0.9715\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0712 - acc: 0.9799 - val_loss: 0.0779 - val_acc: 0.9778\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0501 - acc: 0.9858 - val_loss: 0.0744 - val_acc: 0.9766\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0370 - acc: 0.9895 - val_loss: 0.0678 - val_acc: 0.9789\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0267 - acc: 0.9928 - val_loss: 0.0625 - val_acc: 0.9813\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0205 - acc: 0.9949 - val_loss: 0.0623 - val_acc: 0.9802\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0140 - acc: 0.9971 - val_loss: 0.0617 - val_acc: 0.9805\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0107 - acc: 0.9978 - val_loss: 0.0578 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.0598 - val_acc: 0.9811\n",
      "Baseline Error: 1.89%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Convolutional Neural Network</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.2422 - acc: 0.9312 - val_loss: 0.0695 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.0694 - acc: 0.9795 - val_loss: 0.0467 - val_acc: 0.9854\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.0495 - acc: 0.9854 - val_loss: 0.0372 - val_acc: 0.9883\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0379 - acc: 0.9878 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0314 - acc: 0.9907 - val_loss: 0.0450 - val_acc: 0.9863\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0253 - acc: 0.9924 - val_loss: 0.0359 - val_acc: 0.9877\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0209 - acc: 0.9934 - val_loss: 0.0353 - val_acc: 0.9889\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0320 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0368 - val_acc: 0.9883\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0286 - val_acc: 0.9905\n",
      "CNN Error: 0.95%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Large Convolutional Network </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3786 - acc: 0.8881 - val_loss: 0.0771 - val_acc: 0.9764\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0980 - acc: 0.9701 - val_loss: 0.0580 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0717 - acc: 0.9777 - val_loss: 0.0446 - val_acc: 0.9850\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0593 - acc: 0.9814 - val_loss: 0.0337 - val_acc: 0.9889\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0499 - acc: 0.9846 - val_loss: 0.0326 - val_acc: 0.9884\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0445 - acc: 0.9863 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0399 - acc: 0.9873 - val_loss: 0.0284 - val_acc: 0.9898\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0359 - acc: 0.9889 - val_loss: 0.0301 - val_acc: 0.9895\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0336 - acc: 0.9895 - val_loss: 0.0269 - val_acc: 0.9909\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0301 - acc: 0.9901 - val_loss: 0.0256 - val_acc: 0.9912\n",
      "Large CNN Error: 0.88%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
